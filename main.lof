\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces \textit {The Rhesus Macaque Visual Hierarchy.} Each area depicted here is distinct in cytoarchitecture and maintains a complete retinotopic map. Information flows from the \gls {lgn} (not shown) to V1, then V2, V4, and finally the inferior temporal cortex (IT) in what we call the Ventral stream. \relax }}{2}{figure.caption.15}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textit {Calculating a 1D Tuning Curve.} A.) The process for estimating a single neurons tuning curve to a single stimulus parameter $\theta $. Discrete stimulus values were chosen ($-180^\circ \ by\ 45^\circ \ to 180^\circ $) to span the stimulus space, and repeatedly presented (Gray dots). This provides the expected response to a given stimulus value (dotted line) for the neuron. B.) The estimated tuning and variability for the neuron in panel A.\relax }}{5}{figure.caption.19}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces \textit {Tuning curves and Information.} A.) Von Mises tuning functions with different concentration parameters ($\kappa $). Color indicates highest information tuning functions for fine (Blue) and coarse (red) discrimination. B.) Mutual information (Firing Rate and orientation) as a function of $\kappa $. Blue line depicts $1^\circ $ discrimination, Red line depicts the more realistic situation of 8 stimulus classes ($0^\circ $ to $315^\circ $ in $45^\circ $ steps). \relax }}{6}{figure.caption.23}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces \textit {Gabors as points in a 2D Stimulus Space.} A.) examples of Gabor stimuli with various $\theta $ and spatial frequency values. B.) Positions of the Gabors depicted in panel A within a 2-Dimensional stimulus space. \relax }}{7}{figure.caption.25}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces \textit {Tuning along multiple Feature Dimensions.} A neurons tuning curve, in 2 dimensions becomes a tuning surface. Gray plane represents the spatial frequency value at which the tuning for $\theta $ matches figure \ref {fig:tuningIntro}.\relax }}{9}{figure.caption.26}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces \textit {Population Codes} A.) Tuning curves for 8 homogenous model neurons along the orientation feature dimension. Color at the bottom represents the values of $\theta $. B.) A 2-Dimensional state space plotting the responses of neurons 1\&2 from panel A. Color represents the value of $\theta $ that lead to each pair of neural responses. \relax }}{10}{figure.caption.28}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Cross validation of clustering algorithm. We took 50\% of trials for each neuron, calculated \gls {mi}, ran the clustering algorithm, and minimized AIC to pick the optimum number of clusters. After repeating this process 100 times, we got a distribution of models that supports 3 clusters as optimal for our data.\relax }}{21}{figure.caption.52}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Relationship between Entropy Factor (HF) and Fano Factor (FF). We took the neural responses for our 8 directions of motion (S1), for each neuron in our sample, and plotted FF vs HF. Dotted lines are Poisson processes. Neural responses with a larger lambda parameter (average firing rate) result in a roughly linear relationship between the measures. When neural responses are low, FF overestimates the variability (darker blue dots are highly super-Poisson according to FF, but roughly Poisson according to HF)\relax }}{22}{figure.caption.55}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example and overview of tuning goodness of fit ($R^2$). A Gaussian fit to one example neuron during one bootstrap fold. The full distribution of $R^2$ for our sample of neurons.\relax }}{25}{figure.caption.61}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Average Comparison Effect (CE) during the two windows of interest for neurons' preferred direction and $180^{\circ }$ off preferred (anti-preferred). \relax }}{27}{figure.caption.63}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces {\it Task design and task modulation of MT firing rates.} A.) Trials consisted of a pre-stimulus 1 second fixation period, a first stimulus (S1), followed by a 1.5 second delay, then a second stimulus (S2) and a post-S2 fixation period. After the 1 second post-S2 fixation period subjects were either rewarded (passive task) or had to report a decision with a saccade (active task). During the active task, correct choices were rewarded with juice while incorrect choices resulted in a 3 s time-out signaled by a tone and no reward. During S1, stimuli moved in one of 8 directions ($0^\circ , 45^\circ , 90^\circ , 135^\circ , 180^\circ , 225^\circ , 270^\circ , 315^\circ $), followed by the S2 that moved either in the same direction, or $90^\circ $ off of S1 (rotated left or right). B.) PSTHs of an example neuron recorded in both tasks. Solid line represents the neuron’s response to its preferred direction while dashed lines indicate 180◦ away from preferred (“anti-preferred”). C.) Receptive fields (grey) of each simultaneously recorded neuron from one example session, along with the location and size of the stimuli (red). Black curve corresponds to example neuron in B. Contours represent isointensity contours at $50\%$ of the peak response.\relax }}{29}{figure.caption.66}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces {\it Example Mutual Information Time-Courses.} A.) Mutual information between motion direction and firing rate for an example neuron in the active and passive tasks. B-C.) Same as A for two more example neurons. D.) Task effect on motion information (i.e., MI during active minus MI during passive) during the course of the trial for the three example neurons. Pearson Distance was calculated on these time courses for each pair of neurons. Distance values close to 0 mean similar task effects, whereas distance values close to 1 indicate opposite task effects. We then used the set of pair-wise Pearson distances to cluster neurons based on their task effects for MI.\relax }}{32}{figure.caption.68}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces {\it Motion Information Clustering.} A.) Time series for active minus passive motion information (\gls {mi}) for each neuron. Rows correspond to neurons and color represents the difference in information. See \hyperref [{sec:methods}]{Methods} for the clustering algorithm. B.) Within-cluster averages from panel A.\relax }}{34}{figure.caption.70}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces {\it Effects of task demands differed depending on how informative a neuron was about task-relevant information.} A.) Average raw tuning curves for IDI\ neurons during the active (purple) and passive (green) tasks. Both curves contain the same neurons. Error bars are $\pm SEM$ across neurons. B-C.) Same as A for DDI\ and SDI\ groups respectively. D.) Average trial-to-trial variability during S1 quantified with \gls {hf} (See \hyperref [{sec:methods}]{Methods}). E-F.) Same as panel D for DDI\ neurons and SDI\ groups respectively. G.) Slope of a linear fit to IDI\ neurons' firing rates vs time during a 1s interval before S2 onset in the Active vs Passive tasks. There was a significantly higher ramping during the active task (***: $p=5.66\cdot 10^{-5}$, t-test). H-I.) Same as panel G for DDI\ (ns) and SDI\ (**: $p=0.0098$) groups respectively. J.) Task effect of the width parameter of gaussian fits vs $\Delta MI$ during S1. For each cross validation fold, we fit a gaussian to the 50\% of trials not used for the calculation of \gls {mi} (i.e. the X and Y axes use different trials). We then averaged across folds, and subtracted across task to get a task effect of tuning width. We found no significant relationship between tuning width and $\Delta MI$ ($R^2=0.038, p=0.552$). K.) Same as J but for the height parameter of the gaussian fits ($R^2=0.859, p=3.505 \cdot 10^{-75}$). \relax }}{35}{figure.caption.71}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces {\it DDI\ and SDI\ neurons have different comparison effects across the active and passive tasks.} A.) Time course of comparison effects (AUC between same and different trials within stimulus type) for the IDI\ neurons with significant $S>D$ in either task. B.) Same as panel A but for IDI\ neurons with $D>S$ signal. C-D.) Same as A-B but for DDI\ neurons. E-F.) Same as A-B but for SDI\ neurons. **: $p<0.01$, ***: $p<0.001$, n.s.: not significant t-test for the difference between tasks over the indicated interval. We did not find significant CEs or task effects during the S2 (gray shading) for any group of neurons, but we did see effects during the post-S2 interval for DDI neurons (C-D), and for SDI neurons with $S>D$ CEs (E). \relax }}{38}{figure.caption.74}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces {\it Proportion of neurons with significant comparison signals during active and passive tasks} A.) Proportion of neurons within each information group with significant $S>D$ signal during the S2. B.) Same as panel A for the post-S2\ interval. C) Same as in panel A for the $D>S$ neurons. D) Same as in panel B for the $D>S$ neurons. (***: $p<0.001$, $\chi ^2$ test). E.) Comparison Effects (absolute difference from 0.5 AUC) vs mutual information for each task. Neurons with the largest CE in the active task also conveyed the least direction information. Error bars = 95\% binomial confidence intervals.\relax }}{41}{figure.77}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces {\it The effect of tuning width on \gls {cca}.} Top row.) Example tuning curves for three simulations with descending tuning widths. First column.) Canonical correlation variable pairs for a population of 3 neurons with wide tuning. Neurons were assumed to have the same tuning width but different preferred stimuli along the three dimensions. Second\&Third column.) Same as first column but for medium and narrow tuning respectively.\relax }}{50}{figure.caption.97}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces {\it The effect of irrelevant stimulus dimensions on CCA.} Canonical correlation variable pairs for three models relating neural population space to stimulus space. Model 1.) Similar to models shown in figure \ref {fig:ccaTuning}, with all three stimulus dimensions being relevant to the neurons. Tuning for neurons was set at 50 a.u. across models. Model 2.) Same as model 1, except one stimulus parameter (consistent across neurons) was set to have no impact on the firing rate of the neurons. Model 3.) Same as model 2 but with multiple irrelevant stimulus dimensions.\relax }}{51}{figure.caption.98}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces {\it The effect of Poisson variability on \gls {cca}.} Same as figure \ref {fig:ccaTuning} with the exception of added Poisson variability to the neural responses.\relax }}{52}{figure.caption.100}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces {\it Optimization Example.} This example demonstrates the simplest case of optimization. Some linear, deterministic function (blue line) is sampled repeatedly and the gradient is followed to find the minimum.\relax }}{54}{figure.caption.103}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces {\it Non-Convex Optimization Example.} Here I show how gradient-based optimization can fail in even simple 1D problems. When a function is non-convex (multiple minima/maxima), the local gradient can often be misleading about the global behavior of the function.\relax }}{55}{figure.caption.105}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces {\it Genetic Algorithm Flowchart.} The general process of how a genetic algorithm optimizes.\relax }}{57}{figure.caption.111}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces {\it Example GAN Stimuli.} Four example images generated by the 128 GAN latents (top row) and their pixel reconstruction after dimensionality reduction with PCA (bottom row).\relax }}{72}{figure.caption.145}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces {\it Individual neuron model fits} A.) AIC for GLMs using the GAN latents as predictors (x-axis) and the pixel predictors (y-axis) on neuron spike counts. B.) Relative change in AIC across models for individual neurons. Negative values indicate that the GAN latents are better predictors of neural data while positive values indicate that the pixel model was better. The latent model was better on average in both brain regions (V1: $t=-10.26$, $p=1.54 \cdot 10^{-23}$, V4: $t=-6.02$, $p=3.46\cdot 10^{-9}$; t-test).\relax }}{73}{figure.caption.146}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces {\it Canonical Correlations in neural data} A.) Simulated stimulus data where points were randomly selected to span an arbitrary space. The red line indicates the axis chosen by CCA for panel D. B.) Neural state space of stimuli from panel A (each point is paired with a point from A), where each axis is the firing rate of a neuron. Data was centered for visualization purposes as CCA also centers data. The purple line indicates the axis in neural space chosen by CCA for panel D. C.) Original correlation between neuron 1 (panel B) and latent 1 (panel A). D.) First canonical variable pair from the data in panels A and B. The X and Y axes are now linear combinations of the latent and neural dimensions respectively. \relax }}{75}{figure.caption.147}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces {\it Baseline-Corrected R values} The baseline-corrected r values for each canonical variable pair across brain region and algorithm. Baseline correction was calculated by subtracting out the distribution of r values calculated from randomly permuting predictors and calculating CCA 100 times. The number of significant CCA pairs was calculated using Rao's approximate F statistic.\relax }}{76}{figure.caption.148}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces {\it Linear Relationships in V1} A.) Top CCA pair for a PSO session in V1. Each point is one stimulus and color indicates the normalized $L^2$ norm of the population response vector to that image. B.) Same as panel A for a session optimized with the genetic algorithm. C.) Same as panels A\&B for a session in which no optimization was performed, and instead random images were shown. D-F.) Same as panels A-C except the color now indicates the order of stimuli in the session. G-I.) The GAN latent dimension that CCA found was most linearly related to neural activity for each algorithm. These dimensions correspond to the X-axes in panels A-F.\relax }}{77}{figure.caption.149}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces {\it Linear Relationshipsin V4} A.) Top CCA pair for a PSO session in V4. Each point is one stimulus and color indicates the normalized $L^2$ norm of the population response vector to that image. B.) Same as panel A for a session optimized with the genetic algorithm. C.) Same as panels A\&B for a session in which no optimization was performed, and instead random images were shown. D-F.) Same as panels A-C except the color now indicates the order of stimuli in the session. G-I.) The GAN latent dimension that CCA found was most linearly related to neural activity for each algorithm. These dimensions correspond to the X-axes in panels A-F.\relax }}{78}{figure.caption.150}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces {\it Projections onto CCA1}The value of projections onto Canonical variable pair 1 for each stimulus shown. PSO tends to pick one side of the latent space that it knows has strong optimization evaluations, whereas the genetic algorithm explores more.\relax }}{79}{figure.caption.151}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces {\it DCA pair 1 in V1 across Algorithms} A.) Top DCA pair for a PSO session in V4. Each point is one stimulus and color indicates the normalized $L^2$ norm of the population response vector to that image. B.) Same as panel A for a session optimized with the genetic algorithm. C.) Same as panels A\&B for a session in which no optimization was performed, and instead random images were shown. D-F.) Same as panels A-C except the color now indicates the order of stimuli in the session. G-I.) The GAN latent dimension that DCA found was most strongly related to neural activity for each algorithm. These dimensions correspond to the X-axes in panels A-F.\relax }}{80}{figure.caption.152}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces {\it DCA pair 1 in V4 across Algorithms} A.) Top DCA pair for a PSO session in V4. Each point is one stimulus and color indicates the normalized $L^2$ norm of the population response vector to that image. B.) Same as panel A for a session optimized with the genetic algorithm. C.) Same as panels A\&B for a session in which no optimization was performed, and instead random images were shown. D-F.) Same as panels A-C except the color now indicates the order of stimuli in the session. G-I.) The GAN latent dimension that DCA found was most strongly related to neural activity for each algorithm. These dimensions correspond to the X-axes in panels A-F.\relax }}{81}{figure.caption.153}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces {\it Relative Linearity by Brain region} A.) Linearity of neural/stimulus relationships by DCA dimension pairs. Stimulus parameters used were the GAN latent variables. DCA dimensoin pairs are organized by descending distance covariance and relative linearity was calculated as described in \hyperref [{methods:relativeLinearity}]{Methods}. Each line represents one session, and is colored according to brain area. B.) Same as panel A for pixel predictors.\relax }}{82}{figure.caption.154}%
\addvspace {10\p@ }
