\chapter{\color{thesisBlue} General Discussion} % Main chapter title

\label{ch:generalDiscussion} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
\glsresetall

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


The question of what language the brain uses has as many answers as there are neurons. Each neuron in the visual system receives on the order of 10,000 inputs, but how does the brain decide which neurons connect to which other? We know from developmental studies that axons project along chemical gradients in order to reach target sites (eg. neurons in the LGN project their axons to V1), but one unanswered question is what happens once it is there? It is not possible for thousands of independent chemical gradients to direct billions of neurons with nanometer-level accuracy all simultaneously. I propose here a radical idea; \textbf{it is random.} To be more specific, pseudorandom. Neuroscientists may argue that the macroscopic functional architecture, computational efficiency, and emergent properties of the brain belies randomness as an explanation, but I hope to assuage those critiques. All of these are explainable through other properties such as pruning and synaptic weights after the initial random wiring. There are in fact many reasons that make random-wiring an appealing hypothesis; it is information-efficient, it follows as a microchasm of evolution (the best stimulus embedding will survive), and it is sufficient for animals needs. 

The first requires little explanation, there is a limited amount of genetic data in DNA, certainly fewer than the number of unique synapses in the brain. Compression of a full and detailed brain map containing every synapse is not possible with DNA alone, and yet it is functionally inherited. How is it possible to create something from nothing, to use a blueprint of a kayak to make an aircraft carrier? If every gene in the human body was purely dedicated to directing the formation of synapses, each gene would be responsible for around 3 hundred million neural connections. This is not even considering where on the dendrite to connect or the weight attached to it. From an information compression standpoint, random-wiring is the only plausible solution. In addition to the developmental argument for information-efficiency, the brain must stay flexible enough to change what information is conveyed as need demands. I studied this aspect in chapter \ref{ch:papernak}, where I looked at how feature tuning in the \gls{mt} can change at an individual and population level, depending on behavioral context. I showed that the amount of stimulus information conveyed by \gls{mt} neurons changes depending on the animals goals. Importantly, these types of changes to neuron behavior can happen regardless of the feature basis set used by the brain. The animals task was to differentiate and remember motion direction, but the same results would likely hold true if the relevant feature was a combination of direction, color, speed, etc. This is because "relevant to the animals goals" is decided through experience and does not require a strictly motion-direction-defined neural axis with specific connections to "remember $90^\circ$ motion" neurons. Since rotation of a basis set is a linear operation, then there is some readout of whatever basis set in \gls{mt} that specifies motion direction as the relevant variable with a simple change in perspective. In fact, research has already shown that decision making areas like the \gls{pfc} receive many inputs from sensory areas and therefor use a projection of the sensory data to drive decisions. All I argue here, is that a readout area like \gls{pfc} cares not which basis set the sensory area uses, as mappings can be made between decision and embedding regardless of how the visual features are encoded.

The second argument for random wiring in the brain is that of natural selection. The evolutionary goal of organisms is survival and reproduction. A priori, the brain has no need to differentiate between colors until we view it in the context of survival: Ripe vs. rotten fruit, edible vs. poisonous mushrooms, sick vs. healthy humans. In all of these examples, color is but one aspect of the relevant decision, so why should the brain use pure color as a basis for navigating the world? It would be much more beneficial for the brain to use combinations of feature sets that are more relevant or more prevalent in nature. These bases are combinations of features that can be conceptualized as rotations on the basis set used by mathematicians to describe visual information (color, spacial frequency, luminance, etc.). This would allow the brain to efficiently encode visual information in a language that is, at first glance, difficult to translate. Assume for a moment that the brain does use these rotated basis sets, the question becomes which basis to use (i.e. which features to combine into neurally-relevant axes). This is where I bring it back to the random wiring hypothesis. If the visual system is in fact randomly wired during development, then the combination of features that are relevant to each neuron is also random at first. From there, an elegant combination of "fire together, wire together," and synaptic pruning would result in a sort of dimensionality reduction for stimulus embedding. Visual features in the real world that co-occur would naturally stay together in one latent feature dimension while unused synapses (and therefore visual features) are removed. An example of this is the color green often co-occurs with high-spatial frequency in the form of leaves on a tree or grass in a field while the color blue co-occurs with low-spatial frequency (sky, ocean, etc.). This idea ties in closely to the approaches in chapter \ref{ch:maps}, where rotations of the feature space show strong relationships between features and neural activity. To this end, we sought to maximize the information gained about neural sensory-response functions using a single experiment. We leveraged the rich, continuous, and naturalistic feature space of a \gls{gan} with optimization techniques such as \gls{pso} on large neural ensembles. By optimizing on the $L^2$ norm of the population response vector, we found that both \gls{pso} and genetic algorithms improved the quality of stimulus-response information gained in the experiment defined by stronger linear and nonlinear relationships present between neural responses and \gls{gan} latents. We also found striking similarities across algorithms in the types of features found to be relevant to the neurons, including color palettes, textures and shapes. This provides more evidence of a non-intuitive language used by the brain that is constructed of combinations of visual features, as multiple approaches with different techniques coalesced on similar feature spaces. Interestingly, \gls{gan} latent variables proved to be better at explaining neurons' behavior than even the images themselves at the individual-neuron level. This suggests that the brain may in fact be using some form of latent variable model for representing visual information. So for example, color, may not be relevant to the brain in and of itself, but a nonlinear combination of color, contrast, spatial orientation, etc is a more effective or efficient way to process visual information. As artificial neural networks such as \gls{gan}s take inspiration from the brain, this follows naturally. Another striking finding from this experiment is the degree to which V1 and V4 respond linearly. We found that despite the complexity of stimuli presented in this study V1 still maintained a linear stimulus embedding, while V4 spanned both linear and nonlinear stimulus-response functions. This result held regardless of predictor, in that pixel values and \gls{gan} latent variables had similar trends. 

The third strongest argument for random wiring in the brain is its sufficiency. The core belief of evolution is lazyness; if something works, there is no need to improve on it. In this case, if randomly wiring the brain can support the computational burden necessary, there is no reason to spend extra energy on the problem. It has been shown in modeling experiments that a pseudorandomly connected network of neurons not only exhibits flexible working memory, but mimics the limitations of human working memory \parencite{Bouchacourt2019}. They demonstrated that a network of randomly-connected neurons can hold visual information in working memory, and that interference imposes a limit on the number of items it can remember. I suggest only that the principle they discovered need not be limited to memory systems, but include sensory ones as well. 

Taken together, these studies form the foundation of an approach to sensory coding that takes into account factors that are not typically accounted for in experimental designs by necessity. By accounting for behavioral context (active vs. passive tasks) we demonstrated that the \gls{mt} alters sensory processing in many (even detrimental) ways, but that these effects served to aid other parts of the task. Then, by accounting for untested feature dimensions, we demonstrated that neurons are tuned to complex, nonlinear combinations of features, and that these relationships can be parsed out with the use of artificial intelligence. Future work into the relationship between visual stimuli and neural responses should combine the studies to investigate how high dimensional neural manifold geometry changes with task design. Limitations in the first study (few simultaneously recorded neurons) and the second (no behavioral task) could be removed by investigating how neural populations encode complex feature sets, and how that encoding changes according to subject goals. 

